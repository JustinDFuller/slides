# Deconstructing Go Concurrency Concepts

## Intro

üëã I'm Justin Fuller, a Software Engineer at The New York Times. 

I'm a self-taught developer with about 5 years of experience.

I've worked professionally with Java, Coldfusion, JavaScript, C# .NET, and Go. 

So far, Go is my favorite!

## Warning ‚ÄºÔ∏è

I am not an expert on the internals of the Go Runtime or even concurrent programming.

I have been writing code with these tools for several years.

## Why am I giving this presentation?

I want to demonstrate a learning style.

* Have you ever taken a car engine apart? Radio?
* You learn by investigating how it works, how all the parts fit together.
* This is typically done on things you are already familiar with.
* Gives you the ability to maintain and debug when something goes wrong.
* They are no longer a "black box".

## What is a Concurrency Concept?

* Go has implementations of concepts.
* Concept examples
  * WaitGroup
  * Mutex
  * Pool
  * Channel

## How do you deconstruct a concept?

1. Show
    * Demonstrate the need for the concept with a broken example.

2. Define
    * Agree on a clear explanation of what an implementation of the concept would accomplish.

3. Use
    * If an implementation already exists, start with that.

4. Create
    * After using any existing implementations, write and use your own.

## First, a little code.

At this point, we are naive programmers with no concurrency concepts to aid us.

.play 0-naive-concurrency.go /func main/,

## Our First Concept

The current code doesn't work as expected. All the logs should be printed before "done".

We need a concurrency tool that tracks the number of in-progress operations, then waits for all operations to complete.

We will accomplish this with a __WaitGroup__.

## sync.WaitGroup

Now that we've defined what we need, we use an existing implementation of WaitGroup from Go's `sync` package.

.play 1-waitgroup.go /func main/,

## Custom WaitGroup

We have defined what a waitgroup should do. We have used an existing implementation. It's time to create our own.

.play 2-customwaitgroup.go /type WaitGroup/,

How would you fill this in?

## Filling in the logic 

Could it really be this simple? üôå

.play 3-race-condition-waitgroup.go /type WaitGroup/,

## Race Detector üïµÔ∏è‚Äç‚ôÄÔ∏è

```

‚ùØ go run -race 3-race-condition-waitgroup.go
Saw i = 0
Saw i = 1
==================
WARNING: DATA RACE
Read at 0x00c000136008 by goroutine 7:
  main.(*WaitGroup).Done()
      /Users/justin/code/presentation/3-race-condition-waitgroup.go:29 +0x3a
  main.main.func1()
      /Users/justin/code/presentation/3-race-condition-waitgroup.go:13 +0x123

Previous write at 0x00c000136008 by main goroutine:
  main.(*WaitGroup).Add()
      /Users/justin/code/presentation/3-race-condition-waitgroup.go:25 +0x86
  main.main()
      /Users/justin/code/presentation/3-race-condition-waitgroup.go:9 +0x67

Goroutine 7 (running) created at:
  main.main()
      /Users/justin/code/presentation/3-race-condition-waitgroup.go:10 +0xbd
==================
```

## Demonstrating the race condition

_Note: I would not consider this correct usage of a WaitGroup._

.play 4-demonstrate-race-condition.go /func main/,

## Preventing Race Conditions

It's time to define a new concept before we deconstruct it.

We need to prevent _conflicting access_ to the WaitGroup's counter. To accomplish this we will use a lock, or __Mutex__.

The goal of a mutex is to prevent conflicting access to data, ensuring that updates happen as expected.

## Lock It Down üîí

We've defined a Mutex, now we use an existing implementation. Again, we start with Go's `sync` package.

.play 5-mutex.go /type WaitGroup/,

## What does the race detector say?

```

‚ùØ go run -race 5-mutex.go
Saw i = 0
Added an extra
Saw i = 1
Saw i = 2
Saw i = 3
Saw i = 4
Saw i = 5
Saw i = 6
Saw i = 7
Saw i = 8
Saw i = 9
Saw i = 10
-- skip a few logs --
Saw i = 98
Saw i = 99
Done
```

## Build Your Own

Just like before, we start with an empty shell that fulfills sync.Mutex's signature, but not behavior. The race condition reappears.

.play 6-custom-mutex.go /type Mutex/,

How would you fill this in?

## Filling It In

.play 7-naive-mutex.go /type Mutex/,

## Atomic

From the pkg/sync/atomic documentation:

```

Package atomic provides low-level atomic memory primitives useful for implementing synchronization algorithms.

These functions require great care to be used correctly. Except for special, low-level applications, 
synchronization is better done with channels or the facilities of the sync package. 

Share memory by communicating; don't communicate by sharing memory.
```

## Without a race condition.

.play 8-atomic-mutex.go /const/,

## Can we go any lower?

So far we have gone progressively "lower", first using, then writing our own WaitGroup, then Mutex.

We can't go any lower because the implementations of atomic are written in assembly.

.code 9-atomic.s

This code obtains a lock at the processor level.

## Is that really all there is? (Again)

This code is 100x simpler than the Go standard library code. This can't be all there is.

A little googling and reading through the Go source leads to a few scenarios I didn't think about:

* Starvation
* FIFO

## Starvation?

* The process cannot get the resource it needs to complete a task.
* One goroutine is greedy and gets a lock far more often than others.

Is that happening in my Waitgroup?

```
> go run 10-starvation.go
WaitGroup.Wait() got the lock 17253 times

> GOMAXPROCS=1 go run 10-starvation.go
WaitGroup.Wait() got the lock 6153599 times
```

Wait() got the goroutine, 6,153,599 times. The other goroutines got it 220 times! There's definitely unfairness happening.

## Running with GOMAXPROCS

GOMAXPROCS determines the number of processes Go will use. Setting this to 1 will only allow 1 goroutine to run at a time.

Let's focus on this scenario 

## Classic Go Unfairness Example

.code 10-sync-starvation.go /done :=/,

## Results

With Regular sync.Mutex
```
‚ùØ go run 10-sync-starvation.go
Slow Got 100 Fast Got 10
```

With My Custom Mutex
```
‚ùØ go run 10-sync-starvation.go
Slow Got 21 Fast Got 10
```

GOMAXPROCS=1 sync.Mutex
```
‚ùØ GOMAXPROCS=1 go run 10-sync-starvation.go
Slow Got 103 Fast Got 10
```

GOMAXPROCS=1 custom Mutex
```
‚ùØ GOMAXPROCS=1 go run 10-sync-starvation.go
It never finished, even after several minutes!
```

## Handling 1 Process

.play 11-custom-mutex-starvation.go /Mutex\) Unlock/,

```
‚ùØ GOMAXPROCS=1 go run 11-custom-mutex-starvation.go
Slow got 11 Fast got 10
```

From the docs:

> Gosched yields the processor, allowing other goroutines to run. It does not suspend the current goroutine, so execution resumes automatically.

This mimics what the sync.Mutex does, with one exception. The sync.Mutex only does this when in "starvation mode".

## Back to the original example

Here's what happened when we ran the original WaitGroup with 1 process:
```
> go run 10-starvation.go
WaitGroup.Wait() got the lock 17253 times

> GOMAXPROCS=1 go run 10-starvation.go
WaitGroup.Wait() got the lock 6153599 times
```

Now here's what happens:
```
‚ùØ go run 12-waitgroup-mutex-starvation.go
WaitGroup.Wait() got the lock 2 times.

‚ùØ GOMAXPROCS=1 go run 12-waitgroup-mutex-starvation.go
WaitGroup.Wait() got the lock 2 times.
```

It's definitely more fair!

## Performance Impacts

How does this fairness impact performance?

## FIFO

Is the lock acting as a FIFO queue? Should it?
